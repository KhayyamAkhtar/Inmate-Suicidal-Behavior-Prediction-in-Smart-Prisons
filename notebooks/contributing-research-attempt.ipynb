{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4585027,"sourceType":"datasetVersion","datasetId":2673705}],"dockerImageVersionId":30513,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Installing Packages**","metadata":{}},{"cell_type":"code","source":"removeFile = False\nif removeFile == True:\n    import shutil\n    shutil.rmtree(\"/kaggle/working/anchor\") #removes directory/folder\n\n    import os\n\n    file_to_remove = \"/kaggle/working/anchor.zip\"\n\n    if os.path.exists(file_to_remove):\n        os.remove(file_to_remove)\n        print(f\"{file_to_remove} removed successfully.\")\n    else:\n        print(f\"{file_to_remove} does not exist in the directory.\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T10:11:11.501798Z","iopub.execute_input":"2023-10-25T10:11:11.502221Z","iopub.status.idle":"2023-10-25T10:11:11.534273Z","shell.execute_reply.started":"2023-10-25T10:11:11.502186Z","shell.execute_reply":"2023-10-25T10:11:11.533218Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-25T10:11:11.537192Z","iopub.execute_input":"2023-10-25T10:11:11.537566Z","iopub.status.idle":"2023-10-25T10:11:11.550469Z","shell.execute_reply.started":"2023-10-25T10:11:11.537534Z","shell.execute_reply":"2023-10-25T10:11:11.549214Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/criminal-psychological-data/Criminal Psychological Dataset CSV.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Importing Libraries**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split # Import train_test_split function\nimport time #for calculating execution time\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2023-10-25T10:11:11.551999Z","iopub.execute_input":"2023-10-25T10:11:11.552394Z","iopub.status.idle":"2023-10-25T10:11:12.447226Z","shell.execute_reply.started":"2023-10-25T10:11:11.552364Z","shell.execute_reply":"2023-10-25T10:11:12.445930Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install gdown\n\nimport gdown\n\nrunOriginal = False\n\nif (runOriginal == False): #MODDED RUN\n    # Replace 'FILE_ID' with the actual file ID of your zip folder\n    file_id = '1gdXeAX6YeKop8xHC_CCJh745piebie9e'\n\n    # Define the URL using the file ID\n    url = f'https://drive.google.com/uc?id={file_id}'\n\n    # Download the zip folder\n    output = '/kaggle/working/anchor.zip'  # Replace 'filename' with your desired name\n    gdown.download(url, output, quiet=False)\n\n    import zipfile\n\n    zip_path = '/kaggle/working/anchor.zip'  # Replace 'filename' with the name you used in the previous step\n    extract_path = '/kaggle/working/'  # Replace with the path where you want to extract the contents\n\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_path)\n    \n    print()\n    print(\"Downloaded and extracted modified anchor zip\")\n    print()\nelse: #ORIGINAL RUN\n    # Replace 'FILE_ID' with the actual file ID of your zip folder\n    file_id = '11wfnjytPrSvbfBpYmsAGx5lbybx8xTNx'\n\n    # Define the URL using the file ID\n    url = f'https://drive.google.com/uc?id={file_id}'\n\n    # Download the zip folder\n    output = '/kaggle/working/anchor.zip'  # Replace 'filename' with your desired name\n    gdown.download(url, output, quiet=False)\n\n    import zipfile\n\n    zip_path = '/kaggle/working/anchor.zip'  # Replace 'filename' with the name you used in the previous step\n    extract_path = '/kaggle/working/'  # Replace with the path where you want to extract the contents\n\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_path)\n    \n    print()\n    print(\"Downloaded and extracted original anchor zip\")\n    print()\n\n    \nfrom anchor import utils\nfrom anchor import anchor_tabular","metadata":{"execution":{"iopub.status.busy":"2023-10-25T10:11:12.449314Z","iopub.execute_input":"2023-10-25T10:11:12.450266Z","iopub.status.idle":"2023-10-25T10:11:29.877117Z","shell.execute_reply.started":"2023-10-25T10:11:12.450231Z","shell.execute_reply":"2023-10-25T10:11:29.875780Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.28.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.64.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1gdXeAX6YeKop8xHC_CCJh745piebie9e\nTo: /kaggle/working/anchor.zip\n100%|██████████| 430k/430k [00:00<00:00, 93.0MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\nDownloaded and extracted modified anchor zip\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Data Processing**","metadata":{}},{"cell_type":"code","source":"#reading and preparing datasets\norg_ds = pd.read_csv('/kaggle/input/criminal-psychological-data/Criminal Psychological Dataset CSV.csv')\n\n#converting all columns to lowercase to standardize uss\norg_ds.columns = org_ds.columns.str.lower()\n\n#dataset without non-numeric unwanted fields like date, time, patient number, zero change values\nnumeric_ds = org_ds.drop(['cid','cjdid','staffid','idate','intstrt','instrta','suindt','stjail','pre30jl','pre180jl','cdob',\n                          'chethn1','crace1','c23cmin','c23cmdt','ag1sdin','sdinj30','intend','intenda','editdate','d2bi2pma',\n                          'd2bi2pmb','d2bi2pmc','c18pdmcl','c18pdmcp','c18pdmca','c18spgmc','anytxpm','datadmit','entpro',\n                          'compl','onset1','neworrec','environ','course','worst','worst2','txhispsy','txdralc','hospspec',\n                          'hospmed','othprob3','whowith','currdx','lifedes1','lifedes2','lifedes3','lifetx3','lifedes4',\n                          'lifetx4','lifedes5','lifetx8','screen6a','s61b','s61bs','c17cmdt'] , axis = 1)\n\nprint(\"After removing unimportant fields \" , len(numeric_ds.columns))\n\n#converting categorical values to integer by One Hot Encoding\nnumeric_ds = pd.get_dummies(numeric_ds, columns = ['clive1','job1','majsup1','othoff','othdrug','qtother1',\n                                                             'othtx1','resptx1','a38spgmc','a40spsu','c20spsu','f29spsu',\n                                                             'diff1','diff2','diff3','onset2','new2','othprob1','othprob2',\n                                                             'medvit','alcdrug','freetime','ruleout','lifetx1','lifeage2',\n                                                             'lifetx2','lifetx5','lifedes6','lifetx6','lifedes7','lifetx7','lifedes8'])\n\nprint(\"After converting categorical fields \" , len(numeric_ds.columns))\n\n# replace field that's entirely space (or empty) with NaN\nnumeric_ds = numeric_ds.replace(r'^\\s*$', np.nan, regex=True)\n    \nfor (columnName, columnData) in numeric_ds.iteritems():\n    numeric_ds[columnName] = pd.to_numeric(numeric_ds[columnName])\n    numeric_ds[columnName].fillna(numeric_ds[columnName].mean())\n    numeric_ds[columnName] = numeric_ds[columnName].replace(np.nan, numeric_ds[columnName].mean())\n    numeric_ds[columnName].astype(str).astype(float)\n    \n\n#numeric_ds = numeric_ds.select_dtypes(exclude=['object'])\n\n\n#for (columnName, columnData) in numeric_ds.iteritems():\n#    print(numeric_ds[columnName].value_counts())\n\n#removing special characters from feature names added by One Hot Encoding\nimport re\n# Change columns names ([LightGBM] Do not support special JSON characters in feature name.)\nnew_names = {col: re.sub(r'[^A-Za-z0-9_]+', '', col) for col in numeric_ds.columns}\nnew_n_list = list(new_names.values())\n# [LightGBM] Feature appears more than one time.\nnew_names = {col: f'{new_col}_{i}' if new_col in new_n_list[:i] else new_col for i, (col, new_col) in enumerate(new_names.items())}\nnumeric_ds = numeric_ds.rename(columns=new_names)\n\n\n#for (columnName, columnData) in numeric_ds.iteritems():\n#    print(columnName)\n\n#dataset without features of suicide ideation\nds_woSuicide = numeric_ds.drop(['othoff_SUICIDEATTEMPTS','sidelf','mini4','mini4a','mhsf28a','mhsf29a','gain43',\n                                'gain43a','gain44','gain44a','cursuic1','cursuic2','cursuic3','cursuic4','passuic1',\n                                'passuic2','passuic3','passuic4','currsuic','pastsuic','sciddia1','sciddi1a','scidsev2'], axis = 1)\n\nprint(\"Columns in original dataset without suicide ideation\" , len(ds_woSuicide.columns))\n\n#dataset with features of suicide ideation\nds_wSuicide = numeric_ds\nprint(\"Columns in original dataset with suicide ideation\" , len(ds_wSuicide.columns))\n\n#SHAP 27 features\nrds_woSuicide = ds_woSuicide[['phosplf','scrns3b3','pdborder','mhsftot','scrn3md','suprob2','s65cs','alctype','age1coc',\n                              'age1alc','new2_','age1tob','drgprb3','mhsf35a','hosptime','anxlf','mhsf28','s65a','d6mddlta',\n                              'majsup','arstdrg','depend','drgprb2','scrns3b2','age','tcuds','c1cm30','sattlf']]\n\n#SHAP 27 features\nrds_wSuicide = ds_wSuicide[['mhsf28a','sidelf','gain44','phosplf','age1jl','job','age1coc','drvlic','gain44a','coc6m','aspdtot',\n                            'alctype','age1tob','c7cm6m','mshf10cs','clive','gain53a','suprob2','pdborder','age1mj','drgprb6',\n                            'scrns3c3','arstdrg','allmon','hosptime','gs5a','depend','sattlf']]\n\nprint(\"Columns in reduced dataset without suicide ideation\" , len(rds_woSuicide.columns))\nprint(\"Columns in reduced dataset with suicide ideation\" , len(rds_wSuicide.columns))\n\n#print(len(rds_wSuicide.columns))","metadata":{"execution":{"iopub.status.busy":"2023-10-25T10:11:29.878620Z","iopub.execute_input":"2023-10-25T10:11:29.879279Z","iopub.status.idle":"2023-10-25T10:11:33.154012Z","shell.execute_reply.started":"2023-10-25T10:11:29.879235Z","shell.execute_reply":"2023-10-25T10:11:33.152975Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"After removing unimportant fields  860\nAfter converting categorical fields  2337\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/1557377917.py:29: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for (columnName, columnData) in numeric_ds.iteritems():\n","output_type":"stream"},{"name":"stdout","text":"Columns in original dataset without suicide ideation 2314\nColumns in original dataset with suicide ideation 2337\nColumns in reduced dataset without suicide ideation 28\nColumns in reduced dataset with suicide ideation 28\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Previous without suicide anchor reduced features:\")\nprint(\"phosplf,scrns3b3,scrn3md,pdborder,mhsftot,age,age1coc,hosptime,scrns3b2,mhsf28,mhsf35a,depend,arstdrg,alctype,d6mddlta,age1tob,c1cm30,tcuds,drgprb3,sattlf\")\n      \nprint(\"Previous with suicide anchor reduced features:\")\nprint(\"mhsf28a,sidelf,gain44,pdborder,phosplf,depend,age1tob,arstdrg,gs5a,coc6m,age1mj,age1jl,sattlf\")","metadata":{"execution":{"iopub.status.busy":"2023-10-25T10:11:33.155413Z","iopub.execute_input":"2023-10-25T10:11:33.155721Z","iopub.status.idle":"2023-10-25T10:11:33.161031Z","shell.execute_reply.started":"2023-10-25T10:11:33.155696Z","shell.execute_reply":"2023-10-25T10:11:33.160009Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Previous without suicide anchor reduced features:\nphosplf,scrns3b3,scrn3md,pdborder,mhsftot,age,age1coc,hosptime,scrns3b2,mhsf28,mhsf35a,depend,arstdrg,alctype,d6mddlta,age1tob,c1cm30,tcuds,drgprb3,sattlf\nPrevious with suicide anchor reduced features:\nmhsf28a,sidelf,gain44,pdborder,phosplf,depend,age1tob,arstdrg,gs5a,coc6m,age1mj,age1jl,sattlf\n","output_type":"stream"}]},{"cell_type":"code","source":"#set random_state = 1 for reproducible outcomes\n#DATASET WITHOUT SUICIDE IDEATION\n#,'mhsf28a','mhsf29a'\nrandomState = 3\n#REDUCED DATASET WITHOUT SUICIDE IDEATION\nX_r = rds_woSuicide.drop(['sattlf'], axis=1) #X training features\ny_r = rds_woSuicide.sattlf #y output features\nX_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_r, y_r, test_size=0.2, random_state=randomState) # 80% training and 20% test\n\n#REDUCED DATASET WITH SUICIDE IDEATION\nX_rs = rds_wSuicide.drop(['sattlf'], axis=1) #X training features\ny_rs = rds_wSuicide.sattlf #y output features\nX_train_rs, X_test_rs, y_train_rs, y_test_rs = train_test_split(X_rs, y_rs, test_size=0.2, random_state=randomState) # 80% training and 20% test","metadata":{"execution":{"iopub.status.busy":"2023-10-25T10:11:33.162326Z","iopub.execute_input":"2023-10-25T10:11:33.162662Z","iopub.status.idle":"2023-10-25T10:11:33.179214Z","shell.execute_reply.started":"2023-10-25T10:11:33.162634Z","shell.execute_reply":"2023-10-25T10:11:33.177975Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **XGB Model**","metadata":{}},{"cell_type":"code","source":"#XGB model for anchor\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, recall_score, f1_score\n\ndef XGB(xTrain, xTest, yTrain, yTest, x, pngName):\n    # Create classifer objects\n    xgbModel = xgb.XGBClassifier(objective=\"binary:logistic\", learning_rate=0.3, \n                               booster='gbtree', max_depth=5, n_estimators=100, enable_categorical=False)\n    # Fit\n    xgbModel.fit(xTrain, yTrain)\n    # Predict\n    y_pred = xgbModel.predict(xTest)\n    \n    #Anchor Explainer-------------------------------------------------------------------------\n    #Anchor explaination below\n    # Create a DataFrame with the feature names\n    feature_names = list(xTrain.columns)\n    df = pd.DataFrame(xTrain, columns=feature_names)\n\n    # Create an instance of the AnchorTabularExplainer\n    explainer = anchor_tabular.AnchorTabularExplainer(\n        class_names=['0', '1'],  # List of class names\n        feature_names=feature_names,  # List of feature names\n        categorical_names={},  # Dictionary of categorical feature names\n        train_data=xTrain.values,  # DataFrame of training data\n        discretizer='decile'  # discretization method\n    )\n\n    precision_val = 0\n    coverage_val = 0\n    test_precision_val = 0\n    test_coverage_val = 0\n    mae_val = 0\n    acc_val = 0\n    yPred_instances = []  # Store y_pred_instance values for each instance\n    #explainer.save_to_file(\n    # Open the file in write mode\n    fileDir = '/kaggle/working/' + pngName + '.txt'\n    with open(fileDir, 'w') as f:\n        # Write text to the file\n        for index in range (0, len(xTest)):\n            print(index)\n            print(\"\", file=f)\n            # Choose a prediction to explain\n            idx = index #index of records in testing depends on test/train split. currently 0-70\n            instance = xTest.iloc[[idx]]\n            predict_fn = lambda x: xgbModel.predict(x)\n\n            # Generate an explanation for the chosen prediction\n            explanation = explainer.explain_instance(\n                instance.values[0],\n                predict_fn,\n                threshold=0.95,\n                delta=0.1,  # default is 0.1 - last good result on 0.3\n                tau=0.15  # default is 0.15 - last good result on 0.1\n            )\n            \n            y_pred_instance = predict_fn(instance.values)\n            # Ensure y_pred_instance is a 1-dimensional array\n            y_pred_instance = np.squeeze(y_pred_instance)\n\n            # Get the corresponding true label from yTest and convert to 1-dimensional array\n            y_true = np.array([yTest.iloc[idx]])\n\n            mae = mean_absolute_error(y_true, y_pred_instance.reshape(1, -1))\n            accuracy = accuracy_score(y_true, y_pred_instance.reshape(1, -1))\n\n            acc_val = acc_val + accuracy\n            mae_val = mae_val + mae\n            # Store y_pred_instance for each instance\n            yPred_instances.append(np.ravel(y_pred_instance))\n\n\n            # Print the explanation\n            print('Patient {}:'.format(idx+1), file=f)\n            print('Prediction: {}'.format(y_pred_instance), file=f)\n            #print('Explanation: {}'.format(explanation), file=f)\n            print('Anchor: %s' % (' AND '.join(explanation.names())), file=f)\n            print('Precision: %.2f' % explanation.precision(), file=f)\n            print('Coverage: %.2f' % explanation.coverage(), file=f)\n            #print('Mean Absolute Error (MAE): {}'.format(mae), file=f)\n            print('Accuracy: {}'.format(accuracy), file=f)\n\n\n            # Get test examples where the anchor applies\n            fit_anchor = np.where(np.all(xTest.values[:, explanation.features()] == instance.values[0][explanation.features()], axis=1))[0]\n            print('Anchor test precision: %.2f' % (np.mean(predict_fn(xTest.values[fit_anchor]) == predict_fn(instance.values))), file=f)\n            print('Anchor test coverage: %.2f' % (fit_anchor.shape[0] / float(xTest.shape[0])), file=f)\n\n            precision_val = precision_val + explanation.precision()\n            coverage_val = coverage_val + explanation.coverage()\n            test_precision_val = test_precision_val + np.mean(predict_fn(xTest.values[fit_anchor]) == predict_fn(instance.values))\n            test_coverage_val = test_coverage_val + (fit_anchor.shape[0] / float(xTest.shape[0]))\n\n            \n    # The file is automatically closed when the `with` block is exited\n\n    print(\"!FILE WRITING HAS BEEN COMPLETED!\")\n    num_instances = xTest.shape[0]\n    print(num_instances)\n    precision_val = precision_val / num_instances\n    coverage_val = coverage_val / num_instances\n    test_precision_val = test_precision_val / num_instances\n    test_coverage_val = test_coverage_val / num_instances\n    print(\"Average Precision: \" , round(precision_val, 3))\n    print(\"Average Coverage: \" , round(coverage_val, 3))\n    print(\"Average Test Precision: \" , round(test_precision_val, 3))\n    print(\"Average Test Coverage: \" , round(test_coverage_val, 3))\n    mae_val = mae_val / num_instances\n    acc_val = acc_val / num_instances\n    print(\"Average MAE: \", round(mae_val, 3))\n    print(\"Accuracy: \" , round(acc_val, 3))\n\n    # Create a new yPred based on stored y_pred_instance values\n    yPred_anchor = np.concatenate(yPred_instances)\n    # Calculate and print overall recall and F1 score for entire y_true and yPred\n    recall_val = recall_score(yTest, yPred_anchor)\n    f1_score_val = f1_score(yTest, yPred_anchor)\n\n    print(\"Recall: \" , round(recall_val, 3))\n    print(\"F1 Score: \" , round(f1_score_val, 3))\n\n    \n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2023-10-25T10:11:33.182640Z","iopub.execute_input":"2023-10-25T10:11:33.183236Z","iopub.status.idle":"2023-10-25T10:11:33.204389Z","shell.execute_reply.started":"2023-10-25T10:11:33.183190Z","shell.execute_reply":"2023-10-25T10:11:33.203278Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def timeChange(oldTime, newTime):\n    percentageChange = ((newTime-oldTime)/oldTime) * 100\n    if (percentageChange >= 0):\n        print(f\"{percentageChange:.2f} % increase in time. Not Good.\")\n    else:\n        percentageChange = percentageChange * (-1)\n        print(f\"{percentageChange:.2f} % decrease in time. Good Effort.\")\n        \ndef displayTime(seconds, NameText):\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    remaining_seconds = seconds % 60\n    print(NameText)\n    print(f\"{hours} hours, {minutes} minutes, {remaining_seconds} seconds. (Total {seconds} seconds)\")","metadata":{"execution":{"iopub.status.busy":"2023-10-25T10:11:33.206144Z","iopub.execute_input":"2023-10-25T10:11:33.206478Z","iopub.status.idle":"2023-10-25T10:11:33.218057Z","shell.execute_reply.started":"2023-10-25T10:11:33.206451Z","shell.execute_reply":"2023-10-25T10:11:33.217287Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#running ensemble methods for classifiers\n\nstart_time_noSuicide = time.time()\n\ny_pred_ens_r = XGB(X_train_r, X_test_r, y_train_r, y_test_r, X_r, \"No Suicide\") #reduced dataset\n\nend_time_noSuicide = time.time()\nexecution_time_noSuicide = end_time_noSuicide - start_time_noSuicide\n\ndisplayTime(round(execution_time_noSuicide, 2) , \"Execution time Without Suicide Only:\")\ntimeChange(348.79 , round(execution_time_noSuicide, 2))\n\nstart_time_Suicide = time.time()\n\ny_pred_ens_rs = XGB(X_train_rs, X_test_rs, y_train_rs, y_test_rs, X_rs, \"With Suicide\") #reduced with suicide ideation\n\nend_time_Suicide = time.time()\nexecution_time_Suicide = end_time_Suicide - start_time_Suicide\n\ndisplayTime(round(execution_time_Suicide, 2) , \"Execution time With Suicide Only:\")\ntimeChange(402.74 , round(execution_time_Suicide, 2))\n\nexecution_time_complete = end_time_Suicide - start_time_noSuicide\n\ndisplayTime(round(execution_time_complete, 2) , \"Execution time Total:\")\ntimeChange(751.53 , round(execution_time_complete, 2))","metadata":{"execution":{"iopub.status.busy":"2023-10-25T10:11:33.219397Z","iopub.execute_input":"2023-10-25T10:11:33.219687Z","iopub.status.idle":"2023-10-25T10:18:29.859828Z","shell.execute_reply.started":"2023-10-25T10:11:33.219663Z","shell.execute_reply":"2023-10-25T10:18:29.858496Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n!FILE WRITING HAS BEEN COMPLETED!\n71\nAverage Precision:  0.989\nAverage Coverage:  0.321\nAverage Test Precision:  1.0\nAverage Test Coverage:  0.25\nAverage MAE:  0.099\nAccuracy:  0.901\nRecall:  0.556\nF1 Score:  0.588\nExecution time Without Suicide Only:\n0.0 hours, 3.0 minutes, 31.02000000000001 seconds. (Total 211.02 seconds)\n39.50 % decrease in time. Good Effort.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n!FILE WRITING HAS BEEN COMPLETED!\n71\nAverage Precision:  0.989\nAverage Coverage:  0.534\nAverage Test Precision:  1.0\nAverage Test Coverage:  0.585\nAverage MAE:  0.028\nAccuracy:  0.972\nRecall:  0.889\nF1 Score:  0.889\nExecution time With Suicide Only:\n0.0 hours, 3.0 minutes, 25.610000000000014 seconds. (Total 205.61 seconds)\n48.95 % decrease in time. Good Effort.\nExecution time Total:\n0.0 hours, 6.0 minutes, 56.629999999999995 seconds. (Total 416.63 seconds)\n44.56 % decrease in time. Good Effort.\n","output_type":"stream"}]}]}